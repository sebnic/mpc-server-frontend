# MCP Browser POC - Serveur MCP dans le Navigateur

Ce projet dÃ©montre qu'il est **possible** de faire tourner un serveur MCP (Model Context Protocol) directement dans un navigateur web en utilisant un Web Worker.

## ğŸ¯ Concept

L'architecture est la suivante :
- **Serveur MCP** : Tourne dans un Web Worker (processus isolÃ©)
- **Client MCP** : Tourne dans la page principale
- **Transport** : Communication via `postMessage` au lieu de stdio/SSE traditionnel

## ğŸ—ï¸ Architecture DÃ©taillÃ©e

```mermaid
graph TB
    subgraph Browser["ğŸŒ NAVIGATEUR"]
        subgraph MainThread["ğŸ“± PAGE PRINCIPALE (Main Thread)"]
            UI[UI Components<br/>HTML/CSS/TS]
            Client[MCP Client<br/>- Tool Calling<br/>- Request/Response]
            ClientTransport[WorkerClientTransport<br/>send JSONRPCMessage<br/>onmessage]
            
            UI --> Client
            Client --> ClientTransport
        end
        
        subgraph Worker["âš™ï¸ WEB WORKER (Isolated Thread)"]
            Server[MCP Server<br/>- Tool Registry<br/>- Request Handler]
            LLMService[LLM Service<br/>WebLLM<br/>- Llama 3.2 1B<br/>- WebGPU]
            Agent[LLM Agent<br/>- Function Calling<br/>- Tool Orchestration]
            ServerTransport[WorkerServerTransport<br/>send JSONRPCMsg<br/>onmessage]
            
            Server --> LLMService
            Server --> Agent
            Agent --> LLMService
            ServerTransport --> Server
        end
        
        ClientTransport <-->|postMessage<br/>JSON-RPC| ServerTransport
    end
    
    style MainThread fill:#e3f2fd
    style Worker fill:#f3e5f5
    style Client fill:#90caf9
    style Server fill:#ce93d8
    style LLMService fill:#ffab91
    style Agent fill:#a5d6a7
```

## ğŸ”„ Flux d'une RequÃªte Agent

```mermaid
sequenceDiagram
    actor User
    participant UI as UI Components
    participant Client as MCP Client
    participant Transport as Transport Layer
    participant Server as MCP Server
    participant Agent as LLM Agent
    participant LLM as LLM Service
    participant Tool as Tool (get_time)
    
    User->>UI: "Quelle heure est-il?"
    UI->>Client: callTool('agent_chat', {message: ...})
    Client->>Transport: send(JSONRPCRequest)
    Transport->>Server: postMessage()
    Server->>Agent: agent.chat("Quelle heure est-il?")
    
    Agent->>LLM: Analyse la question
    LLM-->>Agent: {"tool": "get_time", "arguments": {}}
    Note over Agent: DÃ©tecte un tool call
    
    Agent->>Tool: executeTool('get_time', {})
    Tool-->>Agent: "Current time: 14:30:25"
    
    Agent->>LLM: Formule une rÃ©ponse avec le rÃ©sultat
    LLM-->>Agent: "Il est actuellement 14h30"
    
    Agent-->>Server: Response
    Server-->>Transport: JSONRPCResponse
    Transport-->>Client: postMessage()
    Client-->>UI: Result
    UI-->>User: "Il est actuellement 14h30"
    
    rect rgb(200, 230, 255)
        Note over Agent,Tool: Function Calling:<br/>Le LLM dÃ©cide et exÃ©cute<br/>automatiquement les outils
    end
```

## ğŸ“‹ PrÃ©requis

**Important** : Node.js >= 18.0.0 est requis (actuellement vous utilisez Node 14.20.0)

Pour mettre Ã  jour Node.js, vous pouvez utiliser :
```bash
# Avec nvm (recommandÃ©)
nvm install 18
nvm use 18

# Ou avec n
npm install -g n
n stable
```

## ğŸš€ Installation

```bash
npm install
```

## ğŸ’» DÃ©veloppement

```bash
npm run dev
```

Ouvrez votre navigateur Ã  l'adresse indiquÃ©e (gÃ©nÃ©ralement http://localhost:5173)

## ğŸ—ï¸ Build

```bash
npm run build
npm run preview
```

## ğŸ§ª FonctionnalitÃ©s du POC

Le serveur MCP implÃ©mente plusieurs outils de dÃ©monstration :

### ğŸ¤– Outils LLM (WebLLM)
1. **llm_initialize** : Initialise un LLM local (Llama 3.2 1B, ~1GB)
2. **llm_chat** : Discute avec le LLM local
3. **llm_status** : VÃ©rifie le statut du LLM

### ğŸ§  Agent IA (Function Calling)
4. **agent_chat** : Agent intelligent qui peut utiliser les outils MCP automatiquement
5. **agent_reset** : RÃ©initialise l'historique de conversation de l'agent

### ğŸ› ï¸ Outils Utilitaires
6. **get_time** : Retourne l'heure actuelle
7. **echo** : RÃ©pÃ¨te un message
8. **calculate** : Effectue des calculs simples (add, subtract, multiply, divide)

## ğŸ“ Structure du Projet

```
src/
â”œâ”€â”€ transport.ts      # ImplÃ©mentation du transport Web Worker
â”œâ”€â”€ worker.ts         # Serveur MCP dans le Worker
â”œâ”€â”€ client.ts         # Client MCP pour la page principale
â”œâ”€â”€ main.ts           # Application principale
â””â”€â”€ style.css         # Styles
```

## ğŸ”‘ Points ClÃ©s de l'ImplÃ©mentation

### Transport PersonnalisÃ©

Le fichier `transport.ts` implÃ©mente l'interface `Transport` du SDK MCP pour utiliser `postMessage` :

- `WorkerServerTransport` : CÃ´tÃ© serveur (dans le Worker)
- `WorkerClientTransport` : CÃ´tÃ© client (page principale)

### Serveur MCP

Le `worker.ts` crÃ©e un serveur MCP standard avec nos outils personnalisÃ©s.

### Client MCP

Le `client.ts` encapsule la logique de connexion au Worker et l'appel des outils.

## âœ… RÃ©sultat

Ce POC dÃ©montre que :
- âœ… Un serveur MCP peut tourner dans un Web Worker
- âœ… La communication MCP fonctionne via postMessage
- âœ… Le client peut lister et appeler des outils
- âœ… L'architecture MCP est respectÃ©e
- âœ… **Un LLM local peut tourner dans le Worker et Ãªtre exposÃ© via MCP**
- âœ… WebLLM (Llama 3.2 1B) fonctionne entiÃ¨rement dans le navigateur

## ğŸš§ Limitations

- Pas d'accÃ¨s au filesystem natif
- CapacitÃ©s limitÃ©es au contexte navigateur
- Performance potentiellement infÃ©rieure Ã  un serveur Node.js natif

## ğŸ”® Cas d'Usage Potentiels

- Extensions de navigateur avec capacitÃ©s MCP
- Applications web avec agents IA locaux
- Playgrounds/demos MCP
- Tests MCP sans backend
- **Assistants IA 100% locaux et privÃ©s (pas de serveur externe)**
- **Applications offline-first avec IA intÃ©grÃ©e**

## âš ï¸ Note sur WebLLM

Au premier lancement, WebLLM tÃ©lÃ©chargera le modÃ¨le Llama 3.2 1B (~1GB). Le modÃ¨le est mis en cache dans le navigateur pour les utilisations futures. WebGPU est requis (Chrome/Edge rÃ©cent ou Firefox avec flag activÃ©).
