# MCP Browser POC - Serveur MCP dans le Navigateur

Ce projet dÃ©montre qu'il est **possible** de faire tourner un serveur MCP (Model Context Protocol) directement dans un navigateur web en utilisant un Web Worker.

## ğŸ¯ Concept

L'architecture est la suivante :
- **Serveur MCP** : Tourne dans un Web Worker (processus isolÃ©)
- **Client MCP** : Tourne dans la page principale
- **Transport** : Communication via `postMessage` au lieu de stdio/SSE traditionnel

## ğŸ—ï¸ Architecture DÃ©taillÃ©e

```mermaid
graph TB
    subgraph Browser["ğŸŒ NAVIGATEUR"]
        subgraph MainThread["ğŸ“± PAGE PRINCIPALE (Main Thread)"]
            UI[UI Components<br/>HTML/CSS/TS]
            Client[MCP Client<br/>- Tool Calling<br/>- Request/Response]
            ClientTransport[WorkerClientTransport<br/>send JSONRPCMessage<br/>onmessage]
            
            UI --> Client
            Client --> ClientTransport
        end
        
        subgraph Worker["âš™ï¸ WEB WORKER (Isolated Thread)"]
            Server[MCP Server<br/>- Tool Registry<br/>- Request Handler]
            LLMService[LLM Service<br/>WebLLM<br/>- Llama 3.2 1B<br/>- WebGPU]
            Agent[LLM Agent<br/>- Function Calling<br/>- Tool Orchestration]
            ServerTransport[WorkerServerTransport<br/>send JSONRPCMsg<br/>onmessage]
            
            Server --> LLMService
            Server --> Agent
            Agent --> LLMService
            ServerTransport --> Server
        end
        
        ClientTransport <-->|postMessage<br/>JSON-RPC| ServerTransport
    end
    
    style MainThread fill:#e3f2fd
    style Worker fill:#f3e5f5
    style Client fill:#90caf9
    style Server fill:#ce93d8
    style LLMService fill:#ffab91
    style Agent fill:#a5d6a7
```

## ğŸ”„ Flux d'une RequÃªte Agent

```mermaid
sequenceDiagram
    actor User
    participant UI as UI Components
    participant Client as MCP Client
    participant Transport as Transport Layer
    participant Server as MCP Server
    participant Agent as LLM Agent
    participant LLM as LLM Service
    participant Tool as Tool (get_time)
    
    User->>UI: "Quelle heure est-il?"
    UI->>Client: callTool('agent_chat', {message: ...})
    Client->>Transport: send(JSONRPCRequest)
    Transport->>Server: postMessage()
    Server->>Agent: agent.chat("Quelle heure est-il?")
    
    Agent->>LLM: Analyse la question
    LLM-->>Agent: {"tool": "get_time", "arguments": {}}
    Note over Agent: DÃ©tecte un tool call
    
    Agent->>Tool: executeTool('get_time', {})
    Tool-->>Agent: "Current time: 14:30:25"
    
    Agent->>LLM: Formule une rÃ©ponse avec le rÃ©sultat
    LLM-->>Agent: "Il est actuellement 14h30"
    
    Agent-->>Server: Response
    Server-->>Transport: JSONRPCResponse
    Transport-->>Client: postMessage()
    Client-->>UI: Result
    UI-->>User: "Il est actuellement 14h30"
    
    rect rgb(200, 230, 255)
        Note over Agent,Tool: Function Calling:<br/>Le LLM dÃ©cide et exÃ©cute<br/>automatiquement les outils
    end
```

## ğŸ“ PrÃ©requis

**Node.js** : Compatible avec Node 14.18+ (actuellement configurÃ© pour Node 14.20.8)

**âš ï¸ Note importante** :
- âœ… **Node 14.x** : Fonctionne parfaitement avec **Gemini API** (provider recommandÃ©)
- âš ï¸ **Node 14.x** : WebLLM peut avoir des problÃ¨mes de function calling
- âœ… **Node 18+** : Tous les providers fonctionnent complÃ¨tement

**Recommandation** : Utilisez **Gemini comme provider par dÃ©faut** avec Node 14, ou mettez Ã  jour vers Node 18+ si vous souhaitez utiliser WebLLM.

```bash
# Pour mettre Ã  jour vers Node 18+ (optionnel)
# Avec nvm (recommandÃ©)
nvm install 18
nvm use 18
```

## ğŸš€ Installation

```bash
npm install
```

## âš™ï¸ Configuration

Le projet supporte la configuration via variables d'environnement pour sÃ©curiser vos clÃ©s API.

### 1. CrÃ©er le fichier `.env`

```bash
cp .env.example .env
```

### 2. Configurer vos clÃ©s API

Ã‰ditez `.env` et remplissez vos informations :

```env
# Gemini Configuration
VITE_GEMINI_API_KEY=votre_clÃ©_api_gemini_ici
VITE_GEMINI_MODEL=gemini-pro

# WebLLM Configuration (optionnel)
VITE_WEBLLM_MODEL=Llama-3.2-1B-Instruct-q4f32_1-MLC

# Provider par dÃ©faut
VITE_DEFAULT_PROVIDER=gemini
```

### ğŸ”’ SÃ©curitÃ© des ClÃ©s API

Les clÃ©s API configurÃ©es dans `.env` sont :
1. **CryptÃ©es au dÃ©marrage** : Utilise AES-GCM-256 avec Web Crypto API
2. **StockÃ©es en mÃ©moire** : Uniquement sous forme cryptÃ©e
3. **DÃ©cryptÃ©es Ã  la demande** : Seulement quand nÃ©cessaire
4. **LiÃ©es Ã  l'appareil** : Utilise un fingerprint du navigateur comme passphrase

âš ï¸ **Important** : 
- Le fichier `.env` est dans `.gitignore` (ne sera pas commitÃ©)
- N'exposez jamais vos clÃ©s API dans le code
- Pour la production, envisagez un backend proxy

## ğŸ’» DÃ©veloppement

```bash
npm run dev
```

Ouvrez votre navigateur Ã  l'adresse indiquÃ©e (gÃ©nÃ©ralement http://localhost:5173)

## ğŸ—ï¸ Build

```bash
npm run build
npm run preview
```

## ğŸ§ª FonctionnalitÃ©s du POC

Le serveur MCP implÃ©mente plusieurs outils de dÃ©monstration :

### ğŸ¤– Outils LLM (Multi-Provider)
1. **llm_initialize** : Initialise un LLM provider (WebLLM ou Gemini)
2. **llm_chat** : Discute avec le LLM
3. **llm_status** : VÃ©rifie le statut du LLM

**Providers supportÃ©s** :
- **WebLLM** : Llama 3.2 1B local (~1GB), fonctionne 100% dans le navigateur
- **Gemini** : Google Gemini API (nÃ©cessite une clÃ© API)

### ğŸ§  Agent IA (Function Calling)
4. **agent_chat** : Agent intelligent qui peut utiliser les outils MCP automatiquement
5. **agent_reset** : RÃ©initialise l'historique de conversation de l'agent

### ğŸ› ï¸ Outils Utilitaires
6. **get_time** : Retourne l'heure actuelle
7. **echo** : RÃ©pÃ¨te un message
8. **calculate** : Effectue des calculs simples (add, subtract, multiply, divide)

### ğŸ“Š Outils ECharts (Visualisation) âœ¨ NOUVEAU

#### DÃ©couverte HiÃ©rarchique (Token-Efficient)
9. **get_chart_types** : Liste tous les types de graphiques disponibles (line, bar, pie, scatter, radar, gauge, funnel, heatmap)
10. **get_chart_config_schema** : Obtient le schÃ©ma dÃ©taillÃ© pour un type de graphique spÃ©cifique

#### GÃ©nÃ©ration de Graphiques
11. **generate_line_chart** : GÃ©nÃ¨re une configuration pour un graphique en ligne
12. **generate_bar_chart** : GÃ©nÃ¨re une configuration pour un graphique en barres
13. **generate_pie_chart** : GÃ©nÃ¨re une configuration pour un graphique circulaire (support des couleurs personnalisÃ©es)

**GÃ©nÃ©ration par IA** ğŸ¨ :
- **DÃ©crivez votre graphique en langage naturel** et l'IA le gÃ©nÃ¨re automatiquement !
- Exemple : "CrÃ©e un graphique en ligne montrant les ventes de janvier Ã  juin : 120, 200, 150, 180, 220, 250"
- L'agent utilise la dÃ©couverte hiÃ©rarchique (33-56% moins de tokens) :
  - **Quick Path** : Si le type est connu â†’ gÃ©nÃ©ration directe
  - **Discovery Path** : get_chart_types â†’ get_chart_config_schema â†’ generate
- Rendu direct dans le navigateur avec echarts@6.0

**FonctionnalitÃ©s** :
- Interface interactive avec prompt texte pour gÃ©nÃ©ration IA
- Boutons d'exemple pour dÃ©marrage rapide
- DÃ©tection automatique de demandes hors-sujet avec rÃ©ponses humoristiques
- Visualisation immÃ©diate du graphique
- Vue de la configuration JSON gÃ©nÃ©rÃ©e
- Support des couleurs personnalisÃ©es pour les pie charts

ğŸ“– **Guides dÃ©taillÃ©s** : 
- [AI_CHART_GENERATION.md](./AI_CHART_GENERATION.md) - Guide d'utilisation
- [HIERARCHY_DEMO.md](./HIERARCHY_DEMO.md) - SystÃ¨me de dÃ©couverte hiÃ©rarchique

### ğŸ“Š Logging et DÃ©bogage

Le systÃ¨me inclut un logging dÃ©taillÃ© pour suivre les interactions entre le LLM agent et les outils MCP :

**Console du navigateur (F12)** :
```
[Agent] ğŸ’¬ Starting new chat: { message: "Quelle heure est-il?", availableTools: [...] }
[Agent] ğŸ”§ LLM decided to use a tool: { tool: "get_time", arguments: {}, iteration: 1 }
[MCP Server] Received tool call: { tool: "get_time", arguments: {}, timestamp: "..." }
[Agent] âœ… Tool execution completed: { tool: "get_time", resultPreview: "..." }
```

**Points de logging** :
- ğŸ’¬ DÃ©marrage d'une conversation avec la liste des outils disponibles
- ğŸ”§ DÃ©cision du LLM d'utiliser un outil (nom, arguments, itÃ©ration)
- ğŸ“¥ RÃ©ception de la requÃªte cÃ´tÃ© serveur MCP
- âœ… RÃ©sultat de l'exÃ©cution de l'outil
- ğŸ”„ Flux complet du function calling sur plusieurs itÃ©rations

## ğŸ“ Structure du Projet

```
src/
â”œâ”€â”€ transport.ts              # ImplÃ©mentation du transport Web Worker
â”œâ”€â”€ worker.ts                 # Serveur MCP dans le Worker (132 lignes, refactorisÃ©)
â”œâ”€â”€ client.ts                 # Client MCP pour la page principale
â”œâ”€â”€ main.ts                   # Application principale (28 lignes, refactorisÃ©)
â”œâ”€â”€ llm-service.ts            # Service de gestion des LLM providers
â”œâ”€â”€ llm-agent.ts              # Agent IA avec function calling
â”œâ”€â”€ crypto-service.ts         # Chiffrement AES-GCM pour les clÃ©s API
â”œâ”€â”€ config-manager.ts         # Gestion de la configuration
â”œâ”€â”€ ui/                       # ğŸ†• Modules UI (refactorisation)
â”‚   â”œâ”€â”€ logger.ts             # Classe Logger pour les messages UI
â”‚   â”œâ”€â”€ html-template.ts      # Template HTML sÃ©parÃ© de la logique
â”‚   â”œâ”€â”€ event-handlers.ts     # Tous les gestionnaires d'Ã©vÃ©nements
â”‚   â””â”€â”€ style.css             # Styles de l'interface utilisateur
â”œâ”€â”€ charts/                   # ğŸ†• Module de gestion des graphiques
â”‚   â””â”€â”€ chart-handler.ts      # Logique de gÃ©nÃ©ration et rendu ECharts
â”œâ”€â”€ providers/
â”‚   â”œâ”€â”€ llm-provider.interface.ts
â”‚   â”œâ”€â”€ webllm-provider.ts
â”‚   â””â”€â”€ gemini-provider.ts
â””â”€â”€ mcp/                      # ğŸ†• Serveurs MCP modulaires
    â”œâ”€â”€ index.ts              # Export central de tous les serveurs
    â”œâ”€â”€ README.md             # Documentation de l'architecture MCP
    â”œâ”€â”€ basic-tools/          # Outils utilitaires (time, echo, calculate)
    â”‚   â”œâ”€â”€ basic-tools-server.ts
    â”‚   â””â”€â”€ README.md
    â”œâ”€â”€ llm/                  # Outils LLM (initialize, status, chat)
    â”‚   â”œâ”€â”€ llm-server.ts
    â”‚   â””â”€â”€ README.md
    â”œâ”€â”€ agent/                # Agent IA avec function calling
    â”‚   â”œâ”€â”€ agent-server.ts
    â”‚   â””â”€â”€ README.md
    â”œâ”€â”€ echart/               # Serveur ECharts
    â”‚   â”œâ”€â”€ echart-tools.ts   # Outils pour intÃ©gration browser
    â”‚   â”œâ”€â”€ echart-server.ts  # Serveur standalone
    â”‚   â”œâ”€â”€ test-server.ts
    â”‚   â”œâ”€â”€ README.md
    â”‚   â””â”€â”€ STANDALONE.md
    â””â”€â”€ chart-discovery/      # Documentation hiÃ©rarchie dÃ©couverte
        â””â”€â”€ README.md
```

### ğŸ¯ Architecture RefactorisÃ©e

Le projet a Ã©tÃ© entiÃ¨rement refactorisÃ© pour une meilleure maintenabilitÃ© :

- **main.ts** : 821 lignes â†’ **28 lignes** (97% de rÃ©duction)
- **worker.ts** : 843 lignes â†’ **132 lignes** (84% de rÃ©duction)
- **Code modulaire** : Chaque serveur MCP dans son propre dossier
- **Documentation complÃ¨te** : README dans chaque module

ğŸ“– Voir [REFACTORING.md](./REFACTORING.md) pour les dÃ©tails de la refactorisation

## ğŸ”‘ Points ClÃ©s de l'ImplÃ©mentation

### Transport PersonnalisÃ©

Le fichier `transport.ts` implÃ©mente l'interface `Transport` du SDK MCP pour utiliser `postMessage` :

- `WorkerServerTransport` : CÃ´tÃ© serveur (dans le Worker)
- `WorkerClientTransport` : CÃ´tÃ© client (page principale)

### Architecture Modulaire MCP

Le projet utilise une architecture modulaire pour organiser les serveurs MCP :

```typescript
// Chaque module expose son schÃ©ma et son handler
export function getToolsSchema() { return [...]; }
export function handleTool(name, args, deps) {
  // Retourne null si tool non reconnu
  if (name === 'my_tool') return result;
  return null;
}
```

**Avantages** :
- âœ… **SÃ©paration des responsabilitÃ©s** : Chaque serveur dans son module
- âœ… **RÃ©utilisabilitÃ©** : Modules exportables et testables indÃ©pendamment
- âœ… **MaintenabilitÃ©** : Code organisÃ©, facile Ã  naviguer et modifier
- âœ… **ExtensibilitÃ©** : Ajout de nouveaux serveurs sans toucher aux existants

**Organisation** :
- `src/mcp/basic-tools/` : Outils utilitaires (time, echo, calculate)
- `src/mcp/llm/` : Gestion des providers LLM
- `src/mcp/agent/` : Agent IA avec function calling
- `src/mcp/echart/` : GÃ©nÃ©ration de graphiques ECharts
- `src/mcp/index.ts` : Export central de tous les modules

### Serveur MCP

Le `worker.ts` orchestre tous les handlers de maniÃ¨re Ã©lÃ©gante :

```typescript
async function handleToolCall(request) {
  // Essaie chaque handler dans l'ordre
  const basicResult = handleBasicTool(name, args);
  if (basicResult) return basicResult;

  const llmResult = await handleLLMTool(name, args, llmService);
  if (llmResult) return llmResult;

  const agentResult = await handleAgentTool(name, args, agent, tools);
  if (agentResult) return agentResult;
  
  // etc...
  
  throw new Error(`Unknown tool: ${name}`);
}
```

### Client MCP

Le `client.ts` encapsule la logique de connexion au Worker et l'appel des outils.

### Interface Utilisateur

L'UI est maintenant organisÃ©e en modules distincts :
- `ui/logger.ts` : Logging des messages UI avec horodatage
- `ui/html-template.ts` : Template HTML sÃ©parÃ© de la logique
- `ui/event-handlers.ts` : Gestionnaires d'Ã©vÃ©nements organisÃ©s par catÃ©gorie
- `charts/chart-handler.ts` : Logique de gÃ©nÃ©ration et rendu de graphiques

## âœ… RÃ©sultat

Ce POC dÃ©montre que :
- âœ… Un serveur MCP peut tourner dans un Web Worker
- âœ… La communication MCP fonctionne via postMessage
- âœ… Le client peut lister et appeler des outils
- âœ… L'architecture MCP est respectÃ©e
- âœ… **Un LLM local peut tourner dans le Worker et Ãªtre exposÃ© via MCP**
- âœ… WebLLM (Llama 3.2 1B) fonctionne entiÃ¨rement dans le navigateur
- âœ… **Function calling automatique** : l'agent LLM peut orchestrer plusieurs outils MCP
- âœ… **Multi-provider** : Support de WebLLM (local) et Gemini API avec architecture extensible
- âœ… **SÃ©curitÃ©** : ClÃ©s API cryptÃ©es avec AES-GCM-256
- âœ… **ObservabilitÃ©** : Logging dÃ©taillÃ© pour dÃ©boguer le function calling
- âœ… **Architecture modulaire** : Serveurs MCP organisÃ©s en modules rÃ©utilisables
- âœ… **Token-efficient** : DÃ©couverte hiÃ©rarchique des outils (33-56% d'Ã©conomie)
- âœ… **MaintenabilitÃ©** : Code refactorisÃ© avec rÃ©duction de 84-97% de lignes dans les fichiers principaux

## ğŸš§ Limitations

- Pas d'accÃ¨s au filesystem natif
- CapacitÃ©s limitÃ©es au contexte navigateur
- Performance potentiellement infÃ©rieure Ã  un serveur Node.js natif

## ğŸ”® Cas d'Usage Potentiels

- Extensions de navigateur avec capacitÃ©s MCP
- Applications web avec agents IA locaux
- Playgrounds/demos MCP
- Tests MCP sans backend
- **Assistants IA 100% locaux et privÃ©s (pas de serveur externe)**
- **Applications offline-first avec IA intÃ©grÃ©e**
- **Dashboards intelligents avec gÃ©nÃ©ration de graphiques par IA**
- **Outils d'analyse de donnÃ©es interactifs**
- **Serveurs MCP rÃ©utilisables** (chaque module peut Ãªtre extrait et utilisÃ© standalone)

## ğŸ”‘ Configuration des LLM Providers

### Gemini (API) â­ **RecommandÃ©**
- **PrÃ©requis** : ClÃ© API Google (gratuite sur https://makersuite.google.com/app/apikey)
- **Configuration** : Dans `.env` (variable `VITE_GEMINI_API_KEY`)
- **Avantages** : Rapide, puissant, pas de tÃ©lÃ©chargement, **fonctionne parfaitement avec Node 14**
- **InconvÃ©nients** : NÃ©cessite Internet, quotas API
- **Statut** : âœ… **Pleinement opÃ©rationnel avec function calling**

### WebLLM (Local) âš ï¸ **ExpÃ©rimental avec Node 14**
- **PrÃ©requis** : WebGPU (Chrome 113+, Edge, ou Firefox avec flag)
- **Premier lancement** : TÃ©lÃ©charge ~1GB (mis en cache ensuite)
- **Configuration** : Optionnelle dans `.env` (variable `VITE_WEBLLM_MODEL`)
- **Avantages** : 100% local, privÃ©, gratuit
- **InconvÃ©nients** : NÃ©cessite WebGPU, plus lent, **problÃ¨mes de function calling avec Node 14**
- **Statut** : âš ï¸ Fonctionne mieux avec Node 18+ (recommandÃ© uniquement pour tests)

### Obtenir une clÃ© API Gemini

1. Visitez https://makersuite.google.com/app/apikey
2. Connectez-vous avec votre compte Google
3. CrÃ©ez une nouvelle clÃ© API
4. Copiez la clÃ© dans `.env` â†’ `VITE_GEMINI_API_KEY`

## ğŸ¨ Architecture Agnostique

Le systÃ¨me utilise une architecture de **Provider Pattern** :
```typescript
interface LLMProvider {
  initialize(config): Promise<void>;
  chat(messages): Promise<Response>;
  getStatus(): Status;
}
```

### Gestion SÃ©curisÃ©e des Configurations

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              DÃ©marrage Application                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     ConfigManager.initialize()                     â”‚
â”‚  1. Charge variables d'environnement (.env)        â”‚
â”‚  2. GÃ©nÃ¨re passphrase device-specific             â”‚
â”‚  3. Crypte les clÃ©s API (AES-GCM-256)             â”‚
â”‚  4. Stocke en mÃ©moire (forme cryptÃ©e)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Utilisation Runtime                            â”‚
â”‚  - getConfig(): DÃ©crypte Ã  la demande             â”‚
â”‚  - setGeminiApiKey(): Crypte nouvelle clÃ©         â”‚
â”‚  - ClÃ©s jamais en clair dans le code              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Facile d'ajouter d'autres providers (OpenAI, Anthropic, Mistral, etc.) sans modifier le reste du code !
