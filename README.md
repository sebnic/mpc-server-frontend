# MCP Browser POC - Serveur MCP dans le Navigateur

Ce projet dÃ©montre qu'il est **possible** de faire tourner un serveur MCP (Model Context Protocol) directement dans un navigateur web en utilisant un Web Worker.

## ğŸ¯ Concept

L'architecture est la suivante :
- **Serveur MCP** : Tourne dans un Web Worker (processus isolÃ©)
- **Client MCP** : Tourne dans la page principale
- **Transport** : Communication via `postMessage` au lieu de stdio/SSE traditionnel

## ğŸ—ï¸ Architecture DÃ©taillÃ©e

```mermaid
graph TB
    subgraph Browser["ğŸŒ NAVIGATEUR"]
        subgraph MainThread["ğŸ“± PAGE PRINCIPALE (Main Thread)"]
            UI[UI Components<br/>HTML/CSS/TS]
            Client[MCP Client<br/>- Tool Calling<br/>- Request/Response]
            ClientTransport[WorkerClientTransport<br/>send JSONRPCMessage<br/>onmessage]
            
            UI --> Client
            Client --> ClientTransport
        end
        
        subgraph Worker["âš™ï¸ WEB WORKER (Isolated Thread)"]
            Server[MCP Server<br/>- Tool Registry<br/>- Request Handler]
            LLMService[LLM Service<br/>WebLLM<br/>- Llama 3.2 1B<br/>- WebGPU]
            Agent[LLM Agent<br/>- Function Calling<br/>- Tool Orchestration]
            ServerTransport[WorkerServerTransport<br/>send JSONRPCMsg<br/>onmessage]
            
            Server --> LLMService
            Server --> Agent
            Agent --> LLMService
            ServerTransport --> Server
        end
        
        ClientTransport <-->|postMessage<br/>JSON-RPC| ServerTransport
    end
    
    style MainThread fill:#e3f2fd
    style Worker fill:#f3e5f5
    style Client fill:#90caf9
    style Server fill:#ce93d8
    style LLMService fill:#ffab91
    style Agent fill:#a5d6a7
```

## ğŸ”„ Flux d'une RequÃªte Agent

```mermaid
sequenceDiagram
    actor User
    participant UI as UI Components
    participant Client as MCP Client
    participant Transport as Transport Layer
    participant Server as MCP Server
    participant Agent as LLM Agent
    participant LLM as LLM Service
    participant Tool as Tool (get_time)
    
    User->>UI: "Quelle heure est-il?"
    UI->>Client: callTool('agent_chat', {message: ...})
    Client->>Transport: send(JSONRPCRequest)
    Transport->>Server: postMessage()
    Server->>Agent: agent.chat("Quelle heure est-il?")
    
    Agent->>LLM: Analyse la question
    LLM-->>Agent: {"tool": "get_time", "arguments": {}}
    Note over Agent: DÃ©tecte un tool call
    
    Agent->>Tool: executeTool('get_time', {})
    Tool-->>Agent: "Current time: 14:30:25"
    
    Agent->>LLM: Formule une rÃ©ponse avec le rÃ©sultat
    LLM-->>Agent: "Il est actuellement 14h30"
    
    Agent-->>Server: Response
    Server-->>Transport: JSONRPCResponse
    Transport-->>Client: postMessage()
    Client-->>UI: Result
    UI-->>User: "Il est actuellement 14h30"
    
    rect rgb(200, 230, 255)
        Note over Agent,Tool: Function Calling:<br/>Le LLM dÃ©cide et exÃ©cute<br/>automatiquement les outils
    end
```

## ğŸ“‹ PrÃ©requis

**Important** : Node.js >= 18.0.0 est requis (actuellement vous utilisez Node 14.20.0)

Pour mettre Ã  jour Node.js, vous pouvez utiliser :
```bash
# Avec nvm (recommandÃ©)
nvm install 18
nvm use 18

# Ou avec n
npm install -g n
n stable
```

## ğŸš€ Installation

```bash
npm install
```

## âš™ï¸ Configuration

Le projet supporte la configuration via variables d'environnement pour sÃ©curiser vos clÃ©s API.

### 1. CrÃ©er le fichier `.env`

```bash
cp .env.example .env
```

### 2. Configurer vos clÃ©s API

Ã‰ditez `.env` et remplissez vos informations :

```env
# Gemini Configuration
VITE_GEMINI_API_KEY=votre_clÃ©_api_gemini_ici
VITE_GEMINI_MODEL=gemini-1.5-flash

# WebLLM Configuration (optionnel)
VITE_WEBLLM_MODEL=Llama-3.2-1B-Instruct-q4f32_1-MLC

# Provider par dÃ©faut
VITE_DEFAULT_PROVIDER=gemini
```

### ğŸ”’ SÃ©curitÃ© des ClÃ©s API

Les clÃ©s API configurÃ©es dans `.env` sont :
1. **CryptÃ©es au dÃ©marrage** : Utilise AES-GCM-256 avec Web Crypto API
2. **StockÃ©es en mÃ©moire** : Uniquement sous forme cryptÃ©e
3. **DÃ©cryptÃ©es Ã  la demande** : Seulement quand nÃ©cessaire
4. **LiÃ©es Ã  l'appareil** : Utilise un fingerprint du navigateur comme passphrase

âš ï¸ **Important** : 
- Le fichier `.env` est dans `.gitignore` (ne sera pas commitÃ©)
- N'exposez jamais vos clÃ©s API dans le code
- Pour la production, envisagez un backend proxy

## ğŸ’» DÃ©veloppement

```bash
npm run dev
```

Ouvrez votre navigateur Ã  l'adresse indiquÃ©e (gÃ©nÃ©ralement http://localhost:5173)

## ğŸ—ï¸ Build

```bash
npm run build
npm run preview
```

## ğŸ§ª FonctionnalitÃ©s du POC

Le serveur MCP implÃ©mente plusieurs outils de dÃ©monstration :

### ğŸ¤– Outils LLM (Multi-Provider)
1. **llm_initialize** : Initialise un LLM provider (WebLLM ou Gemini)
2. **llm_chat** : Discute avec le LLM
3. **llm_status** : VÃ©rifie le statut du LLM

**Providers supportÃ©s** :
- **WebLLM** : Llama 3.2 1B local (~1GB), fonctionne 100% dans le navigateur
- **Gemini** : Google Gemini API (nÃ©cessite une clÃ© API)

### ğŸ§  Agent IA (Function Calling)
4. **agent_chat** : Agent intelligent qui peut utiliser les outils MCP automatiquement
5. **agent_reset** : RÃ©initialise l'historique de conversation de l'agent

### ğŸ› ï¸ Outils Utilitaires
6. **get_time** : Retourne l'heure actuelle
7. **echo** : RÃ©pÃ¨te un message
8. **calculate** : Effectue des calculs simples (add, subtract, multiply, divide)

### ğŸ“Š Logging et DÃ©bogage

Le systÃ¨me inclut un logging dÃ©taillÃ© pour suivre les interactions entre le LLM agent et les outils MCP :

**Console du navigateur (F12)** :
```
[Agent] ğŸ’¬ Starting new chat: { message: "Quelle heure est-il?", availableTools: [...] }
[Agent] ğŸ”§ LLM decided to use a tool: { tool: "get_time", arguments: {}, iteration: 1 }
[MCP Server] Received tool call: { tool: "get_time", arguments: {}, timestamp: "..." }
[Agent] âœ… Tool execution completed: { tool: "get_time", resultPreview: "..." }
```

**Points de logging** :
- ğŸ’¬ DÃ©marrage d'une conversation avec la liste des outils disponibles
- ğŸ”§ DÃ©cision du LLM d'utiliser un outil (nom, arguments, itÃ©ration)
- ğŸ“¥ RÃ©ception de la requÃªte cÃ´tÃ© serveur MCP
- âœ… RÃ©sultat de l'exÃ©cution de l'outil
- ğŸ”„ Flux complet du function calling sur plusieurs itÃ©rations

## ğŸ“ Structure du Projet

```
src/
â”œâ”€â”€ transport.ts      # ImplÃ©mentation du transport Web Worker
â”œâ”€â”€ worker.ts         # Serveur MCP dans le Worker
â”œâ”€â”€ client.ts         # Client MCP pour la page principale
â”œâ”€â”€ main.ts           # Application principale
â””â”€â”€ style.css         # Styles
```

## ğŸ”‘ Points ClÃ©s de l'ImplÃ©mentation

### Transport PersonnalisÃ©

Le fichier `transport.ts` implÃ©mente l'interface `Transport` du SDK MCP pour utiliser `postMessage` :

- `WorkerServerTransport` : CÃ´tÃ© serveur (dans le Worker)
- `WorkerClientTransport` : CÃ´tÃ© client (page principale)

### Serveur MCP

Le `worker.ts` crÃ©e un serveur MCP standard avec nos outils personnalisÃ©s.

### Client MCP

Le `client.ts` encapsule la logique de connexion au Worker et l'appel des outils.

## âœ… RÃ©sultat

Ce POC dÃ©montre que :
- âœ… Un serveur MCP peut tourner dans un Web Worker
- âœ… La communication MCP fonctionne via postMessage
- âœ… Le client peut lister et appeler des outils
- âœ… L'architecture MCP est respectÃ©e
- âœ… **Un LLM local peut tourner dans le Worker et Ãªtre exposÃ© via MCP**
- âœ… WebLLM (Llama 3.2 1B) fonctionne entiÃ¨rement dans le navigateur
- âœ… **Function calling automatique** : l'agent LLM peut orchestrer plusieurs outils MCP
- âœ… **Multi-provider** : Support de WebLLM (local) et Gemini API avec architecture extensible
- âœ… **SÃ©curitÃ©** : ClÃ©s API cryptÃ©es avec AES-GCM-256
- âœ… **ObservabilitÃ©** : Logging dÃ©taillÃ© pour dÃ©boguer le function calling

## ğŸš§ Limitations

- Pas d'accÃ¨s au filesystem natif
- CapacitÃ©s limitÃ©es au contexte navigateur
- Performance potentiellement infÃ©rieure Ã  un serveur Node.js natif

## ğŸ”® Cas d'Usage Potentiels

- Extensions de navigateur avec capacitÃ©s MCP
- Applications web avec agents IA locaux
- Playgrounds/demos MCP
- Tests MCP sans backend
- **Assistants IA 100% locaux et privÃ©s (pas de serveur externe)**
- **Applications offline-first avec IA intÃ©grÃ©e**

## ğŸ”‘ Configuration des LLM Providers

### WebLLM (Local)
- **PrÃ©requis** : WebGPU (Chrome 113+, Edge, ou Firefox avec flag)
- **Premier lancement** : TÃ©lÃ©charge ~1GB (mis en cache ensuite)
- **Configuration** : Optionnelle dans `.env` (variable `VITE_WEBLLM_MODEL`)
- **Avantages** : 100% local, privÃ©, gratuit
- **InconvÃ©nients** : NÃ©cessite WebGPU, plus lent

### Gemini (API)
- **PrÃ©requis** : ClÃ© API Google (gratuite sur https://makersuite.google.com/app/apikey)
- **Configuration** : Dans `.env` (variable `VITE_GEMINI_API_KEY`)
- **Avantages** : Rapide, puissant, pas de tÃ©lÃ©chargement
- **InconvÃ©nients** : NÃ©cessite Internet, quotas API

### Obtenir une clÃ© API Gemini

1. Visitez https://makersuite.google.com/app/apikey
2. Connectez-vous avec votre compte Google
3. CrÃ©ez une nouvelle clÃ© API
4. Copiez la clÃ© dans `.env` â†’ `VITE_GEMINI_API_KEY`

## ğŸ¨ Architecture Agnostique

Le systÃ¨me utilise une architecture de **Provider Pattern** :
```typescript
interface LLMProvider {
  initialize(config): Promise<void>;
  chat(messages): Promise<Response>;
  getStatus(): Status;
}
```

### Gestion SÃ©curisÃ©e des Configurations

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              DÃ©marrage Application                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     ConfigManager.initialize()                     â”‚
â”‚  1. Charge variables d'environnement (.env)        â”‚
â”‚  2. GÃ©nÃ¨re passphrase device-specific             â”‚
â”‚  3. Crypte les clÃ©s API (AES-GCM-256)             â”‚
â”‚  4. Stocke en mÃ©moire (forme cryptÃ©e)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Utilisation Runtime                            â”‚
â”‚  - getConfig(): DÃ©crypte Ã  la demande             â”‚
â”‚  - setGeminiApiKey(): Crypte nouvelle clÃ©         â”‚
â”‚  - ClÃ©s jamais en clair dans le code              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Facile d'ajouter d'autres providers (OpenAI, Anthropic, Mistral, etc.) sans modifier le reste du code !
